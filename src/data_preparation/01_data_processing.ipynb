{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Processing Pipeline\n",
        "\n",
        "This notebook processes the raw Stack Overflow 2025 Developer Survey data following the methodology from the Medium article example.\n",
        "\n",
        "## Steps:\n",
        "1. Load raw survey data\n",
        "2. Filter and clean data\n",
        "3. Feature engineering\n",
        "4. Encode categorical variables\n",
        "5. Create stratified train/test splits\n",
        "6. Save processed datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import re\n",
        "from pathlib import Path\n",
        "from typing import Optional\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Set paths\n",
        "RAW_DATA = Path(\"../../data/raw/stack-overflow-developer-survey-2025-2/survey_results_public.csv\")\n",
        "INTERIM_OUTPUT = Path(\"../../data/interim/so_2025_clean.csv\")\n",
        "PROCESSED_OUTPUT = Path(\"../../data/processed/so_2025_model_ready.parquet\")\n",
        "TRAIN_SPLIT_OUTPUT = Path(\"../../data/processed/so_2025_train.parquet\")\n",
        "TEST_SPLIT_OUTPUT = Path(\"../../data/processed/so_2025_test.parquet\")\n",
        "FEATURE_METADATA = Path(\"../../data/processed/so_2025_feature_columns.json\")\n",
        "\n",
        "print(\"Imports successful!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration constants\n",
        "USE_COLUMNS = [\n",
        "    \"Country\",\n",
        "    \"EdLevel\",\n",
        "    \"YearsCode\",\n",
        "    \"Employment\",\n",
        "    \"DevType\",\n",
        "    \"ConvertedCompYearly\",\n",
        "    \"RemoteWork\",\n",
        "    \"Currency\",\n",
        "]\n",
        "\n",
        "NA_VALUES = [\"NA\", \"Other (please specify):\"]\n",
        "\n",
        "EDUCATION_CATEGORIES = {\n",
        "    \"Less than secondary\": [\n",
        "        \"Primary/elementary school\",\n",
        "        \"Secondary school (e.g. American high school, German Realschule or Gymnasium, etc.)\",\n",
        "    ],\n",
        "    \"Some college\": [\n",
        "        \"Some college/university study without earning a degree\",\n",
        "        \"Associate degree (A.A., A.S., etc.)\",\n",
        "        \"Professional degree (JD, MD, Ph.D, Ed.D, etc.)\",\n",
        "    ],\n",
        "    \"Bachelor's degree\": [\"Bachelor's degree (B.A., B.S., B.Eng., etc.)\"],\n",
        "    \"Master's degree\": [\"Master's degree (M.A., M.S., M.Eng., MBA, etc.)\"],\n",
        "    \"Post-grad\": [\"Doctoral degree (Ph.D)\"],\n",
        "    \"Self-taught/other\": [\n",
        "        \"Something else\",\n",
        "        \"I prefer not to say\",\n",
        "    ],\n",
        "}\n",
        "\n",
        "REMOTE_CATEGORIES = {\n",
        "    \"Remote\": \"Remote\",\n",
        "    \"Hybrid (some in-person, leans heavy to flexibility)\": \"Hybrid-Flexible\",\n",
        "    \"Hybrid (some remote, leans heavy to in-person)\": \"Hybrid-InPerson\",\n",
        "    \"Your choice (very flexible, you can come in when you want or just as needed)\": \"Hybrid-Choice\",\n",
        "    \"In-person\": \"In-person\",\n",
        "}\n",
        "\n",
        "EMPLOYMENT_ALLOWED = {\n",
        "    \"Employed\",\n",
        "    \"Independent contractor, freelancer, or self-employed\",\n",
        "}\n",
        "\n",
        "print(\"Configuration loaded!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Helper functions\n",
        "def simplify_edlevel(value: Optional[str]) -> Optional[str]:\n",
        "    if pd.isna(value):\n",
        "        return None\n",
        "    for label, raw_values in EDUCATION_CATEGORIES.items():\n",
        "        if value in raw_values:\n",
        "            return label\n",
        "    return value\n",
        "\n",
        "def parse_years_code(value: Optional[str]) -> Optional[float]:\n",
        "    if pd.isna(value):\n",
        "        return None\n",
        "    if isinstance(value, (int, float)):\n",
        "        return float(value)\n",
        "    value = value.strip()\n",
        "    if value.startswith(\"Less than\"):\n",
        "        return 0.5\n",
        "    if value.startswith(\"More than\"):\n",
        "        match = re.search(r\"(\\d+)\", value)\n",
        "        if match:\n",
        "            return float(match.group(1))\n",
        "        return None\n",
        "    try:\n",
        "        return float(value)\n",
        "    except ValueError:\n",
        "        return None\n",
        "\n",
        "def normalize_devtype(value: Optional[str]) -> Optional[str]:\n",
        "    if pd.isna(value):\n",
        "        return None\n",
        "    return value.split(\";\")[0].strip()\n",
        "\n",
        "def normalize_remote(value: Optional[str]) -> Optional[str]:\n",
        "    if pd.isna(value):\n",
        "        return None\n",
        "    return REMOTE_CATEGORIES.get(value, value)\n",
        "\n",
        "print(\"Helper functions defined!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Load Raw Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load raw dataset\n",
        "if not RAW_DATA.exists():\n",
        "    raise FileNotFoundError(f\"Raw dataset not found at {RAW_DATA}\")\n",
        "\n",
        "df_raw = pd.read_csv(RAW_DATA, usecols=USE_COLUMNS, na_values=NA_VALUES)\n",
        "print(f\"Raw dataset loaded: {len(df_raw):,} rows, {len(df_raw.columns)} columns\")\n",
        "print(f\"\\nColumns: {list(df_raw.columns)}\")\n",
        "print(f\"\\nFirst few rows:\")\n",
        "df_raw.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check missing values\n",
        "print(\"Missing values per column:\")\n",
        "print(df_raw.isna().sum())\n",
        "print(f\"\\nMissing value percentages:\")\n",
        "print((df_raw.isna().sum() / len(df_raw) * 100).round(2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Clean Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Start cleaning\n",
        "df_clean = df_raw.copy()\n",
        "\n",
        "# Filter by employment status\n",
        "print(f\"Before employment filter: {len(df_clean):,} rows\")\n",
        "df_clean = df_clean[df_clean[\"Employment\"].isin(EMPLOYMENT_ALLOWED)]\n",
        "print(f\"After employment filter: {len(df_clean):,} rows\")\n",
        "\n",
        "# Apply transformations\n",
        "df_clean[\"EdLevelSimplified\"] = df_clean[\"EdLevel\"].apply(simplify_edlevel)\n",
        "df_clean[\"YearsCodeNum\"] = df_clean[\"YearsCode\"].apply(parse_years_code)\n",
        "df_clean[\"DevTypePrimary\"] = df_clean[\"DevType\"].apply(normalize_devtype)\n",
        "df_clean[\"RemoteCategory\"] = df_clean[\"RemoteWork\"].apply(normalize_remote)\n",
        "\n",
        "# Rename salary column\n",
        "df_clean = df_clean.rename(columns={\"ConvertedCompYearly\": \"CompYearlyUSD\"})\n",
        "\n",
        "print(f\"\\nAfter transformations: {len(df_clean):,} rows\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Drop rows with missing critical values\n",
        "print(f\"Before dropping NA: {len(df_clean):,} rows\")\n",
        "df_clean = df_clean.dropna(\n",
        "    subset=[\n",
        "        \"Country\",\n",
        "        \"CompYearlyUSD\",\n",
        "        \"EdLevelSimplified\",\n",
        "        \"YearsCodeNum\",\n",
        "        \"DevTypePrimary\",\n",
        "    ]\n",
        ")\n",
        "print(f\"After dropping NA: {len(df_clean):,} rows\")\n",
        "\n",
        "# Filter salary range (USD 1k - 600k)\n",
        "print(f\"\\nBefore salary filter: {len(df_clean):,} rows\")\n",
        "df_clean = df_clean[(df_clean[\"CompYearlyUSD\"] >= 1_000) & (df_clean[\"CompYearlyUSD\"] <= 600_000)]\n",
        "print(f\"After salary filter: {len(df_clean):,} rows\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select final columns and add log-transformed salary\n",
        "selected_columns = [\n",
        "    \"CompYearlyUSD\",\n",
        "    \"Country\",\n",
        "    \"EdLevelSimplified\",\n",
        "    \"YearsCodeNum\",\n",
        "    \"Employment\",\n",
        "    \"DevTypePrimary\",\n",
        "    \"RemoteCategory\",\n",
        "]\n",
        "\n",
        "df_clean = df_clean[selected_columns].reset_index(drop=True)\n",
        "df_clean[\"SalaryLog10\"] = np.log10(df_clean[\"CompYearlyUSD\"])\n",
        "\n",
        "print(f\"Final clean dataset: {len(df_clean):,} rows, {len(df_clean.columns)} columns\")\n",
        "print(f\"\\nColumns: {list(df_clean.columns)}\")\n",
        "df_clean.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save clean dataset\n",
        "INTERIM_OUTPUT.parent.mkdir(parents=True, exist_ok=True)\n",
        "df_clean.to_csv(INTERIM_OUTPUT, index=False)\n",
        "print(f\"Clean dataset saved to {INTERIM_OUTPUT}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Encode Features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare features for encoding\n",
        "feature_cols = [\n",
        "    \"Country\",\n",
        "    \"EdLevelSimplified\",\n",
        "    \"YearsCodeNum\",\n",
        "    \"DevTypePrimary\",\n",
        "    \"RemoteCategory\",\n",
        "]\n",
        "\n",
        "cat_cols = [\n",
        "    \"Country\",\n",
        "    \"EdLevelSimplified\",\n",
        "    \"DevTypePrimary\",\n",
        "    \"RemoteCategory\",\n",
        "]\n",
        "\n",
        "num_cols = [\"YearsCodeNum\"]\n",
        "target = df_clean[\"CompYearlyUSD\"].copy()\n",
        "\n",
        "# One-hot encode categorical variables\n",
        "encoded = pd.get_dummies(df_clean[cat_cols], drop_first=True)\n",
        "encoded[num_cols] = df_clean[num_cols]\n",
        "encoded = encoded.fillna(0.0)\n",
        "\n",
        "feature_list = encoded.columns.tolist()\n",
        "\n",
        "print(f\"Encoded features: {len(feature_list)} columns\")\n",
        "print(f\"Sample feature names: {feature_list[:10]}\")\n",
        "encoded.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create processed dataset with target\n",
        "processed = encoded.copy()\n",
        "processed[\"CompYearlyUSD\"] = target\n",
        "\n",
        "PROCESSED_OUTPUT.parent.mkdir(parents=True, exist_ok=True)\n",
        "processed.to_parquet(PROCESSED_OUTPUT, index=False)\n",
        "\n",
        "# Save feature metadata\n",
        "FEATURE_METADATA.write_text(json.dumps({\"feature_columns\": feature_list}, indent=2))\n",
        "\n",
        "print(f\"Encoded dataset saved to {PROCESSED_OUTPUT}\")\n",
        "print(f\"Feature metadata saved to {FEATURE_METADATA}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Create Stratified Train/Test Split\n",
        "\n",
        "For regression problems, we use stratified splitting by binning the target variable to ensure similar salary distributions in train and test sets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create bins for stratified splitting\n",
        "# Use quantiles to create balanced bins\n",
        "n_bins = 10\n",
        "target_binned = pd.qcut(target, q=n_bins, labels=False, duplicates='drop')\n",
        "\n",
        "print(f\"Target distribution across bins:\")\n",
        "print(pd.Series(target_binned).value_counts().sort_index())\n",
        "\n",
        "# Perform stratified split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    encoded,\n",
        "    target,\n",
        "    test_size=0.2,\n",
        "    random_state=450,\n",
        "    shuffle=True,\n",
        "    stratify=target_binned  # Stratify by binned target\n",
        ")\n",
        "\n",
        "print(f\"\\nTrain set: {len(X_train):,} samples\")\n",
        "print(f\"Test set: {len(X_test):,} samples\")\n",
        "print(f\"\\nTrain target stats:\")\n",
        "print(y_train.describe())\n",
        "print(f\"\\nTest target stats:\")\n",
        "print(y_test.describe())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create train and test datasets with target\n",
        "train_split = X_train.copy()\n",
        "test_split = X_test.copy()\n",
        "train_split[\"CompYearlyUSD\"] = y_train\n",
        "test_split[\"CompYearlyUSD\"] = y_test\n",
        "\n",
        "# Save splits\n",
        "TRAIN_SPLIT_OUTPUT.parent.mkdir(parents=True, exist_ok=True)\n",
        "train_split.to_parquet(TRAIN_SPLIT_OUTPUT, index=False)\n",
        "test_split.to_parquet(TEST_SPLIT_OUTPUT, index=False)\n",
        "\n",
        "# Also save as CSV for easier access\n",
        "train_split.to_csv(TRAIN_SPLIT_OUTPUT.with_suffix('.csv'), index=False)\n",
        "test_split.to_csv(TEST_SPLIT_OUTPUT.with_suffix('.csv'), index=False)\n",
        "\n",
        "print(f\"Train split saved to {TRAIN_SPLIT_OUTPUT}\")\n",
        "print(f\"Test split saved to {TEST_SPLIT_OUTPUT}\")\n",
        "print(f\"\\nâœ… Processing complete!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
