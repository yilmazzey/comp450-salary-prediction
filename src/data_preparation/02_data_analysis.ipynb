{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Analysis: Raw and Processed Datasets\n",
        "\n",
        "This notebook provides comprehensive exploratory data analysis (EDA) for:\n",
        "1. **Raw Dataset**: Initial survey data before preprocessing\n",
        "2. **Processed Dataset**: Cleaned and encoded data ready for modeling\n",
        "\n",
        "## Analysis includes:\n",
        "- Data shape and structure\n",
        "- Missing value analysis\n",
        "- Target variable distribution\n",
        "- Categorical feature distributions\n",
        "- Class imbalance checks\n",
        "- Train/test split validation\n",
        "- Feature correlations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "# Set style\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "\n",
        "# Set paths\n",
        "RAW_DATA = Path(\"../../data/raw/stack-overflow-developer-survey-2025-2/survey_results_public.csv\")\n",
        "CLEAN_DATA = Path(\"../../data/interim/so_2025_clean.csv\")\n",
        "PROCESSED_DATA = Path(\"../../data/processed/so_2025_model_ready.parquet\")\n",
        "TRAIN_DATA = Path(\"../../data/processed/so_2025_train.parquet\")\n",
        "TEST_DATA = Path(\"../../data/processed/so_2025_test.parquet\")\n",
        "\n",
        "print(\"Imports successful!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 1: Raw Dataset Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load raw dataset (only relevant columns)\n",
        "USE_COLUMNS = [\n",
        "    \"Country\",\n",
        "    \"EdLevel\",\n",
        "    \"YearsCode\",\n",
        "    \"Employment\",\n",
        "    \"DevType\",\n",
        "    \"ConvertedCompYearly\",\n",
        "    \"RemoteWork\",\n",
        "    \"Currency\",\n",
        "]\n",
        "\n",
        "df_raw = pd.read_csv(RAW_DATA, usecols=USE_COLUMNS, na_values=[\"NA\", \"Other (please specify):\"])\n",
        "\n",
        "print(f\"Raw Dataset Shape: {df_raw.shape}\")\n",
        "print(f\"\\nColumns: {list(df_raw.columns)}\")\n",
        "print(f\"\\nData Types:\")\n",
        "print(df_raw.dtypes)\n",
        "df_raw.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Missing values analysis\n",
        "missing = df_raw.isna().sum()\n",
        "missing_pct = (missing / len(df_raw) * 100).round(2)\n",
        "\n",
        "missing_df = pd.DataFrame({\n",
        "    'Missing Count': missing,\n",
        "    'Missing %': missing_pct\n",
        "}).sort_values('Missing %', ascending=False)\n",
        "\n",
        "print(\"Missing Values in Raw Dataset:\")\n",
        "print(missing_df[missing_df['Missing Count'] > 0])\n",
        "\n",
        "# Visualize missing values\n",
        "if missing_df['Missing Count'].sum() > 0:\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    missing_df[missing_df['Missing Count'] > 0].plot(kind='barh', y='Missing %', legend=False)\n",
        "    plt.title('Missing Values Percentage by Column (Raw Data)')\n",
        "    plt.xlabel('Missing Percentage (%)')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Target variable analysis (salary)\n",
        "salary_col = 'ConvertedCompYearly'\n",
        "valid_salaries = df_raw[salary_col].dropna()\n",
        "\n",
        "print(f\"Valid salary entries: {len(valid_salaries):,} ({len(valid_salaries)/len(df_raw)*100:.1f}%)\")\n",
        "print(f\"\\nSalary Statistics:\")\n",
        "print(valid_salaries.describe())\n",
        "\n",
        "# Distribution plots\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Histogram\n",
        "axes[0].hist(valid_salaries, bins=50, edgecolor='black', alpha=0.7)\n",
        "axes[0].set_title('Salary Distribution (Raw Data)')\n",
        "axes[0].set_xlabel('Annual Salary (USD)')\n",
        "axes[0].set_ylabel('Frequency')\n",
        "axes[0].axvline(valid_salaries.median(), color='red', linestyle='--', label=f'Median: ${valid_salaries.median():,.0f}')\n",
        "axes[0].legend()\n",
        "\n",
        "# Box plot\n",
        "axes[1].boxplot(valid_salaries, vert=True)\n",
        "axes[1].set_title('Salary Box Plot (Raw Data)')\n",
        "axes[1].set_ylabel('Annual Salary (USD)')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Outliers\n",
        "Q1 = valid_salaries.quantile(0.25)\n",
        "Q3 = valid_salaries.quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "outliers = valid_salaries[(valid_salaries < Q1 - 1.5*IQR) | (valid_salaries > Q3 + 1.5*IQR)]\n",
        "print(f\"\\nOutliers (IQR method): {len(outliers):,} ({len(outliers)/len(valid_salaries)*100:.1f}%)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Categorical feature distributions\n",
        "categorical_cols = ['Employment', 'Country', 'EdLevel', 'RemoteWork']\n",
        "\n",
        "for col in categorical_cols:\n",
        "    if col in df_raw.columns:\n",
        "        print(f\"\\n{col} Distribution:\")\n",
        "        value_counts = df_raw[col].value_counts()\n",
        "        print(value_counts.head(10))\n",
        "        print(f\"Unique values: {df_raw[col].nunique()}\")\n",
        "        \n",
        "        # Plot top 10\n",
        "        if len(value_counts) > 10:\n",
        "            top_10 = value_counts.head(10)\n",
        "            plt.figure(figsize=(10, 6))\n",
        "            top_10.plot(kind='barh')\n",
        "            plt.title(f'{col} Distribution (Top 10)')\n",
        "            plt.xlabel('Count')\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "        else:\n",
        "            plt.figure(figsize=(10, 6))\n",
        "            value_counts.plot(kind='barh')\n",
        "            plt.title(f'{col} Distribution')\n",
        "            plt.xlabel('Count')\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 2: Processed Dataset Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load processed datasets\n",
        "df_clean = pd.read_csv(CLEAN_DATA)\n",
        "df_processed = pd.read_parquet(PROCESSED_DATA)\n",
        "df_train = pd.read_parquet(TRAIN_DATA)\n",
        "df_test = pd.read_parquet(TEST_DATA)\n",
        "\n",
        "print(\"Processed Datasets Loaded:\")\n",
        "print(f\"  Clean dataset: {df_clean.shape}\")\n",
        "print(f\"  Processed dataset: {df_processed.shape}\")\n",
        "print(f\"  Train dataset: {df_train.shape}\")\n",
        "print(f\"  Test dataset: {df_test.shape}\")\n",
        "\n",
        "print(f\"\\nClean Dataset Columns:\")\n",
        "print(list(df_clean.columns))\n",
        "df_clean.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for missing values in processed data\n",
        "print(\"Missing Values in Clean Dataset:\")\n",
        "missing_clean = df_clean.isna().sum()\n",
        "print(missing_clean[missing_clean > 0] if missing_clean.sum() > 0 else \"No missing values!\")\n",
        "\n",
        "print(\"\\nMissing Values in Processed Dataset:\")\n",
        "missing_processed = df_processed.isna().sum()\n",
        "print(missing_processed[missing_processed > 0] if missing_processed.sum() > 0 else \"No missing values!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Target variable analysis after preprocessing\n",
        "target_col = 'CompYearlyUSD'\n",
        "\n",
        "print(\"Clean Dataset - Salary Statistics:\")\n",
        "print(df_clean[target_col].describe())\n",
        "\n",
        "# Distribution comparison\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "# Clean dataset distribution\n",
        "axes[0, 0].hist(df_clean[target_col], bins=50, edgecolor='black', alpha=0.7)\n",
        "axes[0, 0].set_title('Salary Distribution (Clean Dataset)')\n",
        "axes[0, 0].set_xlabel('Annual Salary (USD)')\n",
        "axes[0, 0].set_ylabel('Frequency')\n",
        "axes[0, 0].axvline(df_clean[target_col].median(), color='red', linestyle='--', \n",
        "                   label=f'Median: ${df_clean[target_col].median():,.0f}')\n",
        "axes[0, 0].legend()\n",
        "\n",
        "# Log-transformed distribution\n",
        "axes[0, 1].hist(df_clean['SalaryLog10'], bins=50, edgecolor='black', alpha=0.7)\n",
        "axes[0, 1].set_title('Log10(Salary) Distribution')\n",
        "axes[0, 1].set_xlabel('Log10(Annual Salary)')\n",
        "axes[0, 1].set_ylabel('Frequency')\n",
        "\n",
        "# Train/Test comparison\n",
        "axes[1, 0].hist(df_train[target_col], bins=50, alpha=0.7, label='Train', edgecolor='black')\n",
        "axes[1, 0].hist(df_test[target_col], bins=50, alpha=0.7, label='Test', edgecolor='black')\n",
        "axes[1, 0].set_title('Salary Distribution: Train vs Test')\n",
        "axes[1, 0].set_xlabel('Annual Salary (USD)')\n",
        "axes[1, 0].set_ylabel('Frequency')\n",
        "axes[1, 0].legend()\n",
        "\n",
        "# Box plot comparison\n",
        "box_data = [df_train[target_col], df_test[target_col]]\n",
        "axes[1, 1].boxplot(box_data, labels=['Train', 'Test'])\n",
        "axes[1, 1].set_title('Salary Box Plot: Train vs Test')\n",
        "axes[1, 1].set_ylabel('Annual Salary (USD)')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train/Test split validation\n",
        "print(\"Train/Test Split Statistics:\")\n",
        "print(f\"\\nTrain set: {len(df_train):,} samples ({len(df_train)/(len(df_train)+len(df_test))*100:.1f}%)\")\n",
        "print(f\"Test set: {len(df_test):,} samples ({len(df_test)/(len(df_train)+len(df_test))*100:.1f}%)\")\n",
        "\n",
        "print(\"\\nTarget Variable Statistics:\")\n",
        "print(\"\\nTrain:\")\n",
        "print(df_train[target_col].describe())\n",
        "print(\"\\nTest:\")\n",
        "print(df_test[target_col].describe())\n",
        "\n",
        "# Check if distributions are similar (stratified split validation)\n",
        "from scipy import stats\n",
        "ks_statistic, p_value = stats.ks_2samp(df_train[target_col], df_test[target_col])\n",
        "print(f\"\\nKolmogorov-Smirnov Test (Train vs Test):\")\n",
        "print(f\"  KS Statistic: {ks_statistic:.4f}\")\n",
        "print(f\"  p-value: {p_value:.4f}\")\n",
        "print(f\"  Interpretation: {'Distributions are similar' if p_value > 0.05 else 'Distributions differ significantly'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Class Imbalance Analysis\n",
        "\n",
        "For regression, we check for imbalance in:\n",
        "1. Categorical features (Country, Education, DevType, etc.)\n",
        "2. Salary distribution (binned for stratification)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze categorical feature distributions for imbalance\n",
        "categorical_features = ['Country', 'EdLevelSimplified', 'DevTypePrimary', 'RemoteCategory', 'Employment']\n",
        "\n",
        "for col in categorical_features:\n",
        "    if col in df_clean.columns:\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"{col} Distribution:\")\n",
        "        print('='*60)\n",
        "        value_counts = df_clean[col].value_counts()\n",
        "        percentages = (value_counts / len(df_clean) * 100).round(2)\n",
        "        \n",
        "        dist_df = pd.DataFrame({\n",
        "            'Count': value_counts,\n",
        "            'Percentage': percentages\n",
        "        })\n",
        "        print(dist_df)\n",
        "        \n",
        "        # Calculate imbalance metrics\n",
        "        max_pct = percentages.max()\n",
        "        min_pct = percentages.min()\n",
        "        imbalance_ratio = max_pct / min_pct if min_pct > 0 else float('inf')\n",
        "        \n",
        "        print(f\"\\nImbalance Metrics:\")\n",
        "        print(f\"  Most common: {value_counts.index[0]} ({max_pct}%)\")\n",
        "        print(f\"  Least common: {value_counts.index[-1]} ({min_pct}%)\")\n",
        "        print(f\"  Imbalance ratio: {imbalance_ratio:.2f}x\")\n",
        "        print(f\"  {'⚠️ Significant imbalance' if imbalance_ratio > 10 else '✓ Relatively balanced'}\")\n",
        "        \n",
        "        # Visualize\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        if len(value_counts) > 20:\n",
        "            # Show top 20\n",
        "            top_20 = value_counts.head(20)\n",
        "            top_20.plot(kind='barh')\n",
        "            plt.title(f'{col} Distribution (Top 20)')\n",
        "        else:\n",
        "            value_counts.plot(kind='barh')\n",
        "            plt.title(f'{col} Distribution')\n",
        "        plt.xlabel('Count')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Salary distribution binned for stratification analysis\n",
        "n_bins = 10\n",
        "df_clean['SalaryBin'] = pd.qcut(df_clean[target_col], q=n_bins, labels=False, duplicates='drop')\n",
        "\n",
        "print(\"Salary Distribution by Bins (for Stratification):\")\n",
        "bin_counts = df_clean['SalaryBin'].value_counts().sort_index()\n",
        "bin_pct = (bin_counts / len(df_clean) * 100).round(2)\n",
        "\n",
        "bin_df = pd.DataFrame({\n",
        "    'Count': bin_counts,\n",
        "    'Percentage': bin_pct\n",
        "})\n",
        "print(bin_df)\n",
        "\n",
        "# Visualize bin distribution\n",
        "plt.figure(figsize=(12, 6))\n",
        "bin_counts.sort_index().plot(kind='bar')\n",
        "plt.title('Salary Distribution Across Bins (Stratification)')\n",
        "plt.xlabel('Bin Number')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Check bin balance\n",
        "min_bin_pct = bin_pct.min()\n",
        "max_bin_pct = bin_pct.max()\n",
        "balance_ratio = max_bin_pct / min_bin_pct if min_bin_pct > 0 else float('inf')\n",
        "\n",
        "print(f\"\\nBin Balance Metrics:\")\n",
        "print(f\"  Min bin size: {min_bin_pct}%\")\n",
        "print(f\"  Max bin size: {max_bin_pct}%\")\n",
        "print(f\"  Balance ratio: {balance_ratio:.2f}x\")\n",
        "print(f\"  {'⚠️ Some bins are very small' if balance_ratio > 2 else '✓ Bins are well balanced'}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature correlation analysis (for numeric features)\n",
        "numeric_features = ['YearsCodeNum', 'CompYearlyUSD', 'SalaryLog10']\n",
        "\n",
        "corr_matrix = df_clean[numeric_features].corr()\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(corr_matrix, annot=True, fmt='.3f', cmap='coolwarm', center=0, \n",
        "            square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
        "plt.title('Correlation Matrix: Numeric Features')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Correlation with Target (CompYearlyUSD):\")\n",
        "correlations = df_clean[numeric_features].corr()['CompYearlyUSD'].sort_values(ascending=False)\n",
        "print(correlations)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Summary statistics for processed features\n",
        "print(\"Processed Dataset Summary:\")\n",
        "print(f\"  Total features: {df_processed.shape[1] - 1}\")  # Exclude target\n",
        "print(f\"  Total samples: {len(df_processed):,}\")\n",
        "\n",
        "# Check feature types\n",
        "feature_cols = [col for col in df_processed.columns if col != 'CompYearlyUSD']\n",
        "binary_features = [col for col in feature_cols if df_processed[col].nunique() == 2]\n",
        "numeric_features = [col for col in feature_cols if col not in binary_features]\n",
        "\n",
        "print(f\"\\nFeature Types:\")\n",
        "print(f\"  Binary/One-hot encoded: {len(binary_features)}\")\n",
        "print(f\"  Numeric: {len(numeric_features)}\")\n",
        "\n",
        "print(f\"\\nSample binary features: {binary_features[:5]}\")\n",
        "print(f\"\\nNumeric features: {numeric_features}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary of Findings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final summary\n",
        "print(\"=\"*60)\n",
        "print(\"DATA PREPARATION SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(f\"\\n1. Data Reduction:\")\n",
        "print(f\"   Raw dataset: {len(df_raw):,} rows\")\n",
        "print(f\"   Clean dataset: {len(df_clean):,} rows\")\n",
        "print(f\"   Reduction: {(1 - len(df_clean)/len(df_raw))*100:.1f}%\")\n",
        "\n",
        "print(f\"\\n2. Train/Test Split:\")\n",
        "print(f\"   Train: {len(df_train):,} samples ({len(df_train)/(len(df_train)+len(df_test))*100:.1f}%)\")\n",
        "print(f\"   Test: {len(df_test):,} samples ({len(df_test)/(len(df_train)+len(df_test))*100:.1f}%)\")\n",
        "print(f\"   Stratified: ✓ Yes (by salary bins)\")\n",
        "\n",
        "print(f\"\\n3. Target Variable:\")\n",
        "print(f\"   Mean: ${df_clean[target_col].mean():,.0f}\")\n",
        "print(f\"   Median: ${df_clean[target_col].median():,.0f}\")\n",
        "print(f\"   Std Dev: ${df_clean[target_col].std():,.0f}\")\n",
        "print(f\"   Range: ${df_clean[target_col].min():,.0f} - ${df_clean[target_col].max():,.0f}\")\n",
        "\n",
        "print(f\"\\n4. Features:\")\n",
        "print(f\"   Total encoded features: {len(feature_cols)}\")\n",
        "print(f\"   Categorical (one-hot): {len(binary_features)}\")\n",
        "print(f\"   Numeric: {len(numeric_features)}\")\n",
        "\n",
        "print(f\"\\n5. Data Quality:\")\n",
        "print(f\"   Missing values in clean data: {df_clean.isna().sum().sum()}\")\n",
        "print(f\"   Missing values in processed data: {df_processed.isna().sum().sum()}\")\n",
        "\n",
        "print(\"\\n✅ Dataset is ready for modeling!\")\n",
        "print(\"=\"*60)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
